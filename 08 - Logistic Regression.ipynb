{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. design model,inputs and outputs size, and forward pass\n",
    " 2. construct loss and optimizer\n",
    " 3. training loop\n",
    "   - forward pass\n",
    "   - backward pass\n",
    "   - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- with cpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) (569,)\n",
      "float64 int32\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "n_samples: 569, n_features: 30\n",
      "(455, 30) (114, 30) (455,) (114,)\n",
      "(455, 30) (114, 30) (455,) (114,)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'> torch.float32\n",
      "<class 'torch.Tensor'> torch.float32\n",
      "torch.Size([455, 30]) torch.Size([114, 30]) torch.Size([455, 1]) torch.Size([114, 1])\n",
      "epoch: 100: loss: 0.24781327\n",
      "epoch: 200: loss: 0.17749397\n",
      "epoch: 300: loss: 0.14672913\n",
      "epoch: 400: loss: 0.12885503\n",
      "epoch: 500: loss: 0.11694510\n",
      "epoch: 600: loss: 0.10831949\n",
      "epoch: 700: loss: 0.10171279\n",
      "epoch: 800: loss: 0.09644578\n",
      "epoch: 900: loss: 0.09211907\n",
      "epoch: 1000: loss: 0.08848123\n",
      "Time: 1.2732248306274414s\n",
      "accuracy = 0.9298\n"
     ]
    }
   ],
   "source": [
    "# 这里面的data都是叶子节点张量，而requires_grad的都是神经网络里的参数，这些参数会自动赋予grad的\n",
    "# ------------------------\n",
    "# 0. prepare data\n",
    "\n",
    "bc = datasets.load_breast_cancer()   # 乳腺癌 二分类问题，根据输入的特征预测\n",
    "# print(bc)\n",
    "x, y = bc.data, bc.target\n",
    "print(x.shape, y.shape)\n",
    "print(x.dtype, y.dtype)\n",
    "print(type(x), type(y))   # 可以看出属于numpy\n",
    "\n",
    "# x.shape是元祖，以下两种都行，第二种更简便\n",
    "# n_samlpes, n_features = x.shape[0], x.shape[1]\n",
    "n_samlpes, n_features = x.shape\n",
    "print(f'n_samples: {n_samlpes}, n_features: {n_features}')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1234)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# scaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print(type(x_train), type(x_test), type(y_train), type(y_test))\n",
    "\n",
    "# 将numpy转为tensor\n",
    "x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "print(type(x_train), x_train.dtype)\n",
    "print(type(y_train), y_train.dtype) \n",
    "\n",
    "# for i in zip(x_train, x_test, y_train, y_test):\n",
    "#     print(f'\"{i}\": type({i}), {i}.dtype , {i}.shape, \"\\n')\n",
    "\n",
    "# 将y数据转换为列向量\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# 1. model\n",
    "# f = wx + b, sigmoid at the end\n",
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input_features):\n",
    "        super().__init__()\n",
    "        self.Linear = nn.Linear(n_input_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 模型中需要包含激活函数，二分类问题的输出层用sigmoid\n",
    "        # y_predicted = nn.Sigmoid(self.Linear(x))   # nn和torch下面的sigmoid有什么区别？\n",
    "        y_predicted = torch.sigmoid(self.Linear(x))\n",
    "        return y_predicted\n",
    "    \n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# 2. loss and optimizer\n",
    "lr = 0.01\n",
    "criterion = nn.BCELoss()   # 用于二分类问题，每个样本只有两个类别\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# 3. training loop\n",
    "time_begin = time.time()\n",
    "epoch = 1000\n",
    "for i in range(epoch):\n",
    "    # forward pass\n",
    "    y_hat = model(x_train)\n",
    "    loss = criterion(y_hat, y_train)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # grad_zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (i+1) % 100 == 0:   # 是i+1，从0开始，真正的epoch+1\n",
    "        print(f'epoch: {i+1}: loss: {loss.item():.8f}')\n",
    "\n",
    "time_end = time.time()\n",
    "time_ = time_end - time_begin\n",
    "print(f'Time: {time_}s')\n",
    "\n",
    "# 用test数据测试上面的training得到的神经网络的权重\n",
    "# 对于这个例子从技术上讲是不需要的，但一般是加上，因为为了防止后续再对模型进行更新，因为调用了forward，所以张量会存储用于梯度计算\n",
    "with torch.no_grad():\n",
    "    y_hat = model(x_test)\n",
    "    y_hat_ = y_hat.round()   # 输出1还是0，与二分类判断直接联系起来，\n",
    "    acc = y_hat_.eq(y_test).sum() / float(y_test.shape[0])   #判断上一步的值与y_test的值是否一致，一致就记为1，然后总和除以数据的长度，就是正确率；用float是为了保证足够精确的小数\n",
    "    print(f'accuracy = {acc:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- with cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) (569,)\n",
      "float64 int32\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "n_samples: 569, n_features: 30\n",
      "(455, 30) (114, 30) (455,) (114,)\n",
      "(455, 30) (114, 30) (455,) (114,)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'> torch.float32\n",
      "<class 'torch.Tensor'> torch.float32\n",
      "torch.Size([455, 30]) torch.Size([114, 30]) torch.Size([455, 1]) torch.Size([114, 1])\n",
      "epoch: 100: loss: 0.24877240\n",
      "epoch: 200: loss: 0.17892997\n",
      "epoch: 300: loss: 0.14756942\n",
      "epoch: 400: loss: 0.12910828\n",
      "epoch: 500: loss: 0.11677261\n",
      "epoch: 600: loss: 0.10785934\n",
      "epoch: 700: loss: 0.10106193\n",
      "epoch: 800: loss: 0.09566969\n",
      "epoch: 900: loss: 0.09126195\n",
      "epoch: 1000: loss: 0.08757325\n",
      "Time: 2.158806324005127s\n",
      "accuracy = 0.9474\n"
     ]
    }
   ],
   "source": [
    "# 设置device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# 不能用下面这行代码，它不能让torch自动的将所有张量和模型放到GPU中\n",
    "# torch.set_default_device('cuda')\n",
    "# ------------------------\n",
    "# 0. prepare data\n",
    "\n",
    "bc = datasets.load_breast_cancer()   # 乳腺癌 二分类问题，根据输入的特征预测\n",
    "# print(bc)\n",
    "x, y = bc.data, bc.target\n",
    "print(x.shape, y.shape)\n",
    "print(x.dtype, y.dtype)\n",
    "print(type(x), type(y))   # 可以看出属于numpy\n",
    "\n",
    "# x.shape是元祖，以下两种都行，第二种更简便\n",
    "# n_samlpes, n_features = x.shape[0], x.shape[1]\n",
    "n_samlpes, n_features = x.shape\n",
    "print(f'n_samples: {n_samlpes}, n_features: {n_features}')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1234)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# scaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print(type(x_train), type(x_test), type(y_train), type(y_test))\n",
    "\n",
    "# 将numpy转为tensor\n",
    "x_train = torch.from_numpy(x_train.astype(np.float32)).to(device)\n",
    "x_test = torch.from_numpy(x_test.astype(np.float32)).to(device)\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32)).to(device)\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32)).to(device)\n",
    "print(type(x_train), x_train.dtype)\n",
    "print(type(y_train), y_train.dtype) \n",
    "\n",
    "# for i in zip(x_train, x_test, y_train, y_test):\n",
    "#     print(f'\"{i}\": type({i}), {i}.dtype , {i}.shape, \"\\n')\n",
    "\n",
    "# 将y数据转换为列向量\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# 1. model\n",
    "# f = wx + b, sigmoid at the end\n",
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input_features):\n",
    "        super().__init__()\n",
    "        self.Linear = nn.Linear(n_input_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 模型中需要包含激活函数，二分类问题的输出层用sigmoid\n",
    "        # y_predicted = nn.Sigmoid(self.Linear(x))   # nn和torch下面的sigmoid有什么区别？\n",
    "        y_predicted = torch.sigmoid(self.Linear(x))\n",
    "        return y_predicted\n",
    "    \n",
    "model = LogisticRegression(n_features).to(device)\n",
    "\n",
    "# 2. loss and optimizer\n",
    "lr = 0.01\n",
    "criterion = nn.BCELoss()   # 用于二分类问题，每个样本只有两个类别\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# 3. training loop\n",
    "time_begin = time.time()\n",
    "epoch = 1000\n",
    "for i in range(epoch):\n",
    "    # forward pass\n",
    "    y_hat = model(x_train)\n",
    "    loss = criterion(y_hat, y_train)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # grad_zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (i+1) % 100 == 0:   # 是i+1，从0开始，真正的epoch+1\n",
    "        print(f'epoch: {i+1}: loss: {loss.item():.8f}')\n",
    "\n",
    "time_end = time.time()\n",
    "time_ = time_end - time_begin\n",
    "print(f'Time: {time_}s')\n",
    "\n",
    "# 用test数据测试上面的training得到的神经网络的权重\n",
    "# 不计算梯度，对于这个例子从技术上讲是不需要的，但一般是加上\n",
    "# 减少计算资源，可以防止后续再对模型进行更新，因为调用了forward，所以张量会存储用于梯度计算\n",
    "with torch.no_grad():\n",
    "    y_hat = model(x_test)\n",
    "    y_hat_ = y_hat.round()\n",
    "    acc = y_hat_.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
