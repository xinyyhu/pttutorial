{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9881, -0.7402, -0.2209], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#要算梯度，需要把\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([18., 18., 18.], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(18., device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#计算图，注意grad_fn和Add/Mul backward0\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "z = 2 * y * y\n",
    "print(z)\n",
    "\n",
    "zz = z.mean()\n",
    "print(zz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3492, 1.6798, 2.3722], device='cuda:0')\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\_device.py:77: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#计算gradient,会自动计算zz关于x的梯度，并存储在x.grad中\n",
    "#x是叶子节点，默认只计算叶子节点的梯度\n",
    "zz.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.], device='cuda:0', requires_grad=True)\n",
      "tensor([3., 3., 3.], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(3., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor([0.3333, 0.3333, 0.3333], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#计算图，注意grad_fn和Add/Mul backward0\n",
    "x = torch.ones(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "loss = y.mean()\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "print(x.grad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.], device='cuda:0', requires_grad=True)\n",
      "tensor([3., 3., 3.], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(3., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor([0.3333, 0.3333, 0.3333], device='cuda:0')\n",
      "tensor([0.3333, 0.3333, 0.3333], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#计算图，注意grad_fn和Add/Mul backward0\n",
    "x = torch.ones(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "#对非叶子节点张量（non-leaf）保留梯度！！！\n",
    "y.retain_grad()\n",
    "\n",
    "loss = y.mean()\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讲到了jacobian matrix，乘积J⋅v表示的是函数L关于x的梯度，J的每一行和v的逐元素相乘（这里面表示了通过不同的中间变量y，对每个x分量的路径不同，因此不能约掉），再相加，得到L对每个x分量的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#from pytorch documentation:is_leaf\n",
    "#可以清晰的知道梯度和叶子张量的关系，就是只要是自定义的、定义了grad需求的且没进行过计算的，都是叶子张量\n",
    "\n",
    "a = torch.rand(10, requires_grad=True)\n",
    "print(a.is_leaf)\n",
    "b = torch.rand(10, requires_grad=True).cuda()\n",
    "print(b.is_leaf)\n",
    "c = torch.rand(10, requires_grad=True) + 2\n",
    "print(c.is_leaf)\n",
    "d = torch.rand(10).cuda()\n",
    "print(d.is_leaf)\n",
    "e = torch.rand(10).cuda().requires_grad_()\n",
    "print(e.is_leaf)\n",
    "f = torch.rand(10, requires_grad=True, device=\"cuda\")\n",
    "print(f.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5495,  0.0919, -1.6931], requires_grad=True)\n",
      "tensor([ 1.5495,  0.0919, -1.6931])\n",
      "tensor([ 1.5495,  0.0919, -1.6931])\n",
      "tensor([3.5495, 2.0919, 0.3069])\n"
     ]
    }
   ],
   "source": [
    "#阻止pytorch跟踪历史？计算grad_fn贡献\n",
    "#在training loop中，更新权重，这个操作不应该称为梯度计算的一部分？\n",
    "\n",
    "\n",
    "#x.requires_grad_(False)\n",
    "#x.detach(),创建一个新tensor不需要梯度\n",
    "#with torch.no_grad():，wrap（包装）\n",
    "\n",
    "\n",
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "#一共有三种方法，可以阻止创建梯度和计算中的跟踪历史\n",
    "\n",
    "# No.1: modify the variable in-place，后续不计算梯度\n",
    "x.requires_grad_(False)   \n",
    "print(x)\n",
    "\n",
    "# No 2: 停止追踪梯度，不改变现有张量，得到新张量\n",
    "y = x.detach()  \n",
    "print(y)\n",
    "\n",
    "# No 3: 这样做是为了不影响自动梯度计算图\n",
    "with torch.no_grad():\n",
    "    z = x + 2\n",
    "    print(z)\n",
    "# equal to ->\n",
    "# z = x + 2\n",
    "# z.requires_grad_(False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights grad:  tensor([3., 3., 3., 3.], device='cuda:0')\n",
      "weight:  tensor([1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n",
      "weights grad:  tensor([6., 6., 6., 6.], device='cuda:0')\n",
      "weight:  tensor([1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# a training example\n",
    "# 这个例子再说明，如果每个epoch后不对梯度进行清零的话，梯度会累加，得到了错误的结果\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(2):\n",
    "    model_output = (weights * 3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(\"weights grad: \", weights.grad)\n",
    "    print(\"weight: \", weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight:  tensor([1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n",
      "weights grad:  tensor([3., 3., 3., 3.], device='cuda:0')\n",
      "weights grad:  tensor([0., 0., 0., 0.], device='cuda:0')\n",
      "weight:  tensor([1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n",
      "weights grad:  tensor([3., 3., 3., 3.], device='cuda:0')\n",
      "weights grad:  tensor([0., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# a training example for zero_grad!! \n",
    "# # 梯度清零这非常重要！！！\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(2):\n",
    "    model_output = (weights * 3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(\"weight: \", weights)\n",
    "    print(\"weights grad: \", weights.grad)\n",
    "    \n",
    "    weights.grad.zero_()\n",
    "    print(\"weights grad: \", weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " weight:  tensor([1., 1., 1., 1.], device='cuda:0', requires_grad=True)\n",
      "weights grad:  tensor([3., 3., 3., 3.], device='cuda:0')\n",
      "weight:  tensor([0.9700, 0.9700, 0.9700, 0.9700], device='cuda:0', requires_grad=True)\n",
      "weights grad:  tensor([0., 0., 0., 0.], device='cuda:0')\n",
      "\n",
      " weight:  tensor([0.9700, 0.9700, 0.9700, 0.9700], device='cuda:0', requires_grad=True)\n",
      "weights grad:  tensor([3., 3., 3., 3.], device='cuda:0')\n",
      "weight:  tensor([0.9400, 0.9400, 0.9400, 0.9400], device='cuda:0', requires_grad=True)\n",
      "weights grad:  tensor([0., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# a training example for zero_grad!! \n",
    "# # 梯度更新 + 梯度清零\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "for epoch in range(2):\n",
    "    model_output = (weights * 3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(\"\\n\", \"weight: \", weights)\n",
    "    print(\"weights grad: \", weights.grad)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        weights -= lr * weights.grad\n",
    "    print(\"weight: \", weights)\n",
    "    \n",
    "    weights.grad.zero_()\n",
    "    print(\"weights grad: \", weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(weights, lr=0.01)\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "weights.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Step 1: Define your model architecture\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Step 2: Define your loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Step 3: Initialize an optimizer\n",
    "model = MyModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Step 4: Perform forward pas\n",
    "inputs = torch.randn(1, 10)\n",
    "outputs = model(inputs)\n",
    "\n",
    "# Step 5: Calculate the loss\n",
    "target = torch.randn(1, 1)\n",
    "loss = loss_fn(outputs, target)\n",
    "\n",
    "# Step 6: Perform backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Step 7: Update the model parameters using the optimizer\n",
    "optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
